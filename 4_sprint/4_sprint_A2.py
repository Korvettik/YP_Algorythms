# https://contest.yandex.ru/contest/24414/run-report/148391449/

# -- ПРИНЦИП РАБОТЫ ---
#(1)
# При вводе данных, сразу создаем словарь словарей. В нем храним как ключи все слова,
# что подаются на вход. Дальше будем работать только с ним. Значения ключей это также словарь,
# где ключи - порядковые номера предложений. где встретилось слово. а значение - количество раз.
#(2)
# Затем проходимcя по множеству-уникальных слов каждого запроса и находим для него перечень
# уникальных номеров предложений, где фигурируют слова из этого запроса. Это важный момент.
# Мы для каждого запроса ограничили зону поиска предложений.
#(3)
# Финально для этого конкретного запроса проходимся по его словам, выдергиваем соответствующие слова
# из словаря словарей и суммируем только те значения количеств, где слово соответствует
# номеру предложения данного запроса.

# -- ДОКАЗАТЕЛЬСТВА КОРРЕКТНОСТИ --
# Фактически мы перебираем все слова всех предложений. что падаются на входе со всеми словами запросов.
# Только для каждого запроса мы работаем с конкретным и ограниченным перечнем предложений, а не каждый раз со всем.

# -- ОПРЕДЕЛЕНИЕ СЛОЖНОСТИ АЛГОРИТМА --
# Временная сложность очень грубо O(n) - чтобы построить главный общий словарь словарей.
# Пространственная сложность O(n)  - на операции хранения элементов в словаре словарей, входящих запросов



from collections import defaultdict, Counter

def find_relevant_documents(n, letter_list, m, query_letter_list):
    """
    Для каждого запроса выведите на одной строке номера пяти самых релевантных документов.
    Если нашлось менее пяти документов, то выведите столько, сколько нашлось.
    Документы с релевантностью 0 выдавать не нужно.

    Подумайте над случаями, когда запросы состоят из слов,
    встречающихся в малом количестве документов.

    Что если одно слово много раз встречается в одном документе?
    """


    # (1) ЗА ОДИН ПРОХОД ПО ВХОДНЫМ ДАННЫМ СОЗДАЕМ ЕДИНЫЙ СЛОВАРЬ ПО ВСЕМ УНИКАЛЬНЫМ СЛОВАМ,
    # где ключ - само слово, а значение также словарик, где ключ - номер предложения, где встретилось слово, а значение - количество раз

    # Создаем один общий пустой словарь.
    # Если в словаре не будет найден какой-либо ключ, то он будет создан со значением по умолчанию  - тут dict - пустой словарь.
    # т.е. это будет словарь словарей    {'i': {0: 1}, 'love': {0: 1}, 'coffee': {0: 1, 1: 1}}
    index = defaultdict(dict)
    #print(f'index {index}')

    # проходимся по кортежам (номер предложения, список слов предложения)
    for doc_id, letter_words in enumerate(letter_list):

        # Автоподсчет количества встречаемых слов (словарь, где ключ - уникальное слово, значение - сколько раз оно встретилось в подаваемом предложении)
        letter_word_counts_dict = Counter(letter_words)
        #print(f'letter_word_counts_dict {letter_word_counts_dict}')

        # добавляем в общий словарь статистику для слова + где и сколько раз оно встречается
        for word, count in letter_word_counts_dict.items():
            index[word][doc_id] = count
            #print(index)

    # итого мы имеем что-то такое
    # index = {'i': {0: 1}, 'love': {0: 1}, 'coffee': {0: 1, 1: 1}}




    results = []  # будущий общий результат всех запросов


    # (2) РАБОТАЕМ С УНИКАЛЬНЫМИ СЛОВАМИ КАЖДОГО ЗАПРОСА

    # берем уникальные (множество) слова ПЕРВОГО КОНКРЕТНОГО запроса
    for letter_query_set in query_letter_list:
        #print(letter_query_set)


        # (3) ВАЖНЫЙ МОМЕНТ! ДЛЯ КОНКРЕТНОГО ЗАПРОСА СОБИРАЕМ КОНКРЕТНЫЙ ПЕРЕЧЕНЬ НОМЕРОВ ДОКУМЕНТОВ, ГДЕ ВСТРЕЧАЮТСЯ ЕГО СЛОВА.
        # Для того, чтобы работать только с нужными предложениями, а не всеми

        # именно множество номеров предложений, т.к. мы собираем только номера предложений,
        # а разные слова могут встретиться в одинаковых номерах предложений
        finded_letters_id = set()

        for word in letter_query_set:
            if word in index:  # слово из запроса может и не быть в общем словаре входных предложений
                finded_letters_id.update(index[word].keys())


        # (4) ПРОХОДИМСЯ ПО ВСЕМ НОМЕРАМ ОТОБРАННЫХ ПРЕДЛОЖЕНИЙ КОНКРЕТНОГО ЗАПРОСА и СКЛАДЫВАЕМ КОЛИЧЕСТВА (релевантность)
        scores = []  # список кортежей
        for letter_id in finded_letters_id:  # множество отобранных предложений первого запроса
            relevance = 0  # будущая общая накопительная релевантность конкретного слова
            for word in letter_query_set:  # слова множества запроса
                if (word in index and   # слово из запроса может и не быть в общем словаре входных предложений
                        letter_id in index[word]):  # БЕРЕМ ТОЛЬКО тот элемент {0: 1, 1: 1} --- где ключ нужного нам номера предложения
                    relevance += index[word][letter_id]

            # (5) отбираем только те номера предложений конкретного запроса, где есть вхождения
            if relevance > 0:
                # добавляем кортежами, чтобы затем фильтровать по ним
                scores.append((-relevance, letter_id))


        # (6)
        # Сортируем и берем топ-5
        scores.sort()
        top_letters = []
        for score, letter_id in scores[:5]:
            # Номера документов в выводе начинаются с 1
            top_letters.append(str(letter_id + 1))

        results.append(" ".join(top_letters))

    # Вывод результатов
    for result in results:
        print(result)







if __name__ == '__main__':

    n = int(input().strip())  # количество документов в базе

    letter_list = []  # список с подаваемыми предложениями
    for i in range(n):
        letter = input().strip().split(' ')
        letter_list.append(letter)  # список списков


    m = int(input().strip())  # число запросов
    query_letter_list = []  # список с подаваемыми запросами
    for i in range(m):
        letter_set = set(input().strip().split(' '))
        query_letter_list.append(letter_set)  # записываем предложение запроса как множество со словами


    #print(f'letter_list {letter_list}')
    #print(f'query_letter_list {query_letter_list}')

    find_relevant_documents(n, letter_list, m, query_letter_list)