# https://contest.yandex.ru/contest/24414/run-report/149030803/

# -- ПРИНЦИП РАБОТЫ ---
#(1)
# При вводе данных, сразу создаем словарь словарей. В нем храним как ключи все слова,
# что подаются на вход. Дальше будем работать только с ним. Значения ключей это также словарь,
# где ключи - порядковые номера предложений. где встретилось слово. а значение - количество раз.
#(2)
# Отфильтровываем те слова конкретного запроса, что встречаются в общем списке слов (их может и не быть)
#(3)
# Проходимся по отобранным словам конкретного запроса. Группируем по номерам документов. Считаем накопительную релевантность.

# -- ДОКАЗАТЕЛЬСТВА КОРРЕКТНОСТИ --
# Фактически мы перебираем все слова всех предложений. что падаются на входе со всеми словами запросов.
# Только для каждого запроса мы работаем с конкретным и ограниченным перечнем предложений, а не каждый раз со всем.

# -- ОПРЕДЕЛЕНИЕ СЛОЖНОСТИ АЛГОРИТМА --
"""
1) Парсинг данных (ввод)
Читаем список документов в штуках - значит О(n)
Кажный документ разбиваем на слова - значит О(l)
Соответственно в общем эта процедура займет О(n*l)

2) Строим индекс (делаем множество уникальных слов из всех полученных данных)
Проходим по всем документам - значит О(n)
Делаем подсчет количества встречаемых слов для документа - значит О(l)
Соответственно в общем эта процедура займет О(n*l)

3) Подготовка выдачи (для одного запроса)
Фильтруем до состояния уникальности слова конкретного запроса,
проходимся по всем словам запроса + проверяем его вхождение в общий индекс слов - О(q)
Для каждого отфильтрованного слова запроса проходимся по значениям этого же слова, но уже из индекса слов,
и формируем словарь подсчета общей релевантности по документам - О(k)
Соответственно в общем эта процедура займет О(q*k)

4) Сортировка и вывод
Сначала создаем список оценок, который зависит от существующего объема - О(r)
Сортируем список встроенной функцией - О(r* log r)
Отбираем нужный срез по количеству - O(1)


Итого:
Один раз получаем данные и создаем список --- О(n*l)
Для каждого запроса --- О(m-число запросов * (q*k + r* log r))

Худший случай, когда очень много релевантных документов (r==n) + слова запроса часто встречаются (k==n)
Тогда для каждого запроса --- О(m-число запросов * (q*n + n* log n))

Лучший случай, когда очень мало релевантных документов (r==1) + слова запроса редко встречаются (k==1)
Тогда для каждого запроса --- О(m-число запросов * q)

"""




from collections import defaultdict, Counter

def building_index(n, letter_list):
    """Отдельно создаем индекс всех-всех слов"""
    # (1) ЗА ОДИН ПРОХОД ПО ВХОДНЫМ ДАННЫМ СОЗДАЕМ ЕДИНЫЙ СЛОВАРЬ ПО ВСЕМ УНИКАЛЬНЫМ СЛОВАМ,
    # где ключ - само слово, а значение также словарик, где ключ - номер предложения, где встретилось слово, а значение - количество раз
    # {'i': {0: 1}, 'love': {0: 1}, 'coffee': {0: 1, 1: 1}}

    index = defaultdict(dict)

    # проходимся по кортежам (номер предложения, список слов предложения)
    for doc_id, letter_words in enumerate(letter_list):

        # Автоподсчет количества встречаемых слов (словарь, где ключ - уникальное слово, значение - сколько раз оно встретилось в подаваемом предложении)
        letter_word_counts_dict = Counter(letter_words)

        # добавляем в общий словарь статистику для слова + где и сколько раз оно встречается
        for word, count in letter_word_counts_dict.items():
            index[word][doc_id] = count

    return index



def find_relevant_documents(index, m, query_letter_set):
    """
    Для каждого запроса выведите на одной строке номера пяти самых релевантных документов.
    Если нашлось менее пяти документов, то выведите столько, сколько нашлось.
    Документы с релевантностью 0 выдавать не нужно.

    Подумайте над случаями, когда запросы состоят из слов,
    встречающихся в малом количестве документов.

    Что если одно слово много раз встречается в одном документе?
    """


    # (2) Фильтруем слова запроса. Что они действительно есть в общем списке слов
    finded_letters_words = set()  # итоговое множество слов запроса, что нашлись в общих словах
    for word in query_letter_set:
        if word in index:  # слово из запроса может и не быть в общем словаре входных предложений
            finded_letters_words.add(word)


    # (3) Для каждого отфильтрованного слова запроса проходимся по тем документам, где оно встречается и собираем счетчики
    scores_dict = defaultdict(int)  # ключ - номер документа, значение - накопительная релевантность
    for word in finded_letters_words:  # слова отфильтрованного множества запроса
        for doc_id, count in index[word].items():
            scores_dict[doc_id] += count

    # (4) перегоняем элементы словаря в список кортежей, нулевые не берем
    scores = []
    for doc_id, relevance in scores_dict.items():
        scores.append((-relevance, doc_id))


    # (5)
    # Сортируем и берем топ-5
    scores.sort()
    top_letters = []
    for score, letter_id in scores[:5]:
        # Номера документов в выводе начинаются с 1
        top_letters.append(str(letter_id + 1))

    print(*top_letters)








if __name__ == '__main__':

    n = int(input().strip())  # количество документов в базе

    letter_list = []  # список с подаваемыми предложениями
    for i in range(n):
        letter = input().strip().split(' ')
        letter_list.append(letter)  # список списков

    index = building_index(n, letter_list)

    m = int(input().strip())  # число запросов
    query_letter_list = []  # список с подаваемыми запросами
    for i in range(m):
        query_letter_set = set(input().strip().split(' '))  # ЗДЕСЬ УЖЕ МНОЖЕСТВО - УНИКАЛЬНЫЕ СЛОВА ЗАПРОСА
        # прочитанный запрос СРАЗУ обрабатываем
        find_relevant_documents(index, m, query_letter_set)